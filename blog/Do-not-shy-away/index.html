<!doctype html>
<html>
<head>
<title>Do not shy away</title>
<meta name=description content="üì¨ julian's stuff">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/julian.css">
<link rel="icon" href="/img/julian.ico">
<style>
    #main p {
        text-align: justify;
    }
</style>

</head>
<body>
    <div class="flex_container">
        <div class="menu_bar">
            <a href="/" id="me"><img src="/img/graphy.png" style="height: 100px;"></a>
        </div>
        <div class="center_bar" id="titlebar">
            <div class="title">Do not shy away</div>
        </div>
        <div class="menu_bar" id="topright"></div>
    </div>
    <div class="flex_container">
        <div class="menu_bar" id="menu">
            <ul>
                <a href="/"><li>about</li></a>
                <a href="/contact"><li>contact</li></a>
                <a href="/updates/"><li>updates</li></a>
                <a href="/sketches/"><li>sketches</li></a>
                <a href="/photos/"><li>photos</li></a>
            </ul>
        </div>
        <div class="center_bar" id="main">
<p style="margin-top: 0">
Do not shy away from the hard problems in deep learning.
<p>
<p>We have made tremendous progress as a society on modeling the
sequence of language. By scaling next-token prediction, we have achieved
highly useful language-based assistants. Now, there lies a new, even
more exciting frontier in AI: modeling the sequence of the world. At
each moment in time, humans move through a rich, multimodal sequence of
sensory stimuli and motor control. Taking inspiration from the success
of next-token prediction, what if we could train next-<em>moment</em>
prediction models which learn to generate streams of continuous
visuomotor data?</p>
<p>A highly realistic, realtime world simulator. One day, at hundreds of
frames per second, predicting photorealistic egocentric images and raw
audio, conditioned on a long, dense history of frames spanning hours ‚Äì
maybe years ‚Äì into the past. Importantly, predicting not just
observations, but also dextrous human motor actions and <em>speech</em>
in one, unified model. I take the extreme position here and advocate for
the ultimate instantiation of sequence modeling: predict the data. All
of it.</p>
<p>What lies before us is the hardest problem in deep learning.
Importantly, this model subsumes language models. This is not an
argument <em>against</em> today‚Äôs language models, however ‚Äì it is
instead a full embrace of their success. Let us take loving inspiration
from language modeling and apply its best ideas to the most general
case: end-to-end autoregressive modeling of human experience, behavior,
and language ‚Äì in the raw, continuous data space. And, yes, supervised
pretraining followed by reinforcement learning, except now the
environment is planet Earth. If you like working on language models,
you‚Äôll <em>love</em> working on this.</p>
<p>Yes, if successful, the training and inference of such a model would
require more compute than today‚Äôs frontier language models. <em>Orders
of magnitude more.</em></p>
<p>‚ÄúUm, is that even possible?‚Äù, I hear you say. ‚ÄúWe will not be able to
fabricate enough chips to train such a model, let alone generate enough
power across the whole planet to keep them running. Why not opt for a
more efficient approach? We don‚Äôt <em>have</em> to do realtime
inference, we don‚Äôt <em>have</em> to store every frame in the context,
we can just use the diffusion model as a ‚Äòrendering layer‚Äô, we can just
use a language model for high level planning, we could take a
hierarchical System 1 System 2 approach, you shouldn‚Äôt take the bitter
lesson so literally, besides, video prediction will never achieve
realistic physics, oh and <em>world models</em> are just supposed to
predict in latent sp-‚Äù</p>
<p>I say we do it anyway. Believe it or not, people said <a
href="https://news.ycombinator.com/item?id=23895481">similar things</a>
about language models when GPT-3 came out. That large language models
would be far too computationally expensive and cost far too much energy
to be practical. That GPT models would never power NPCs in video games
because they could never be run locally or be served fast enough. That
they don‚Äôt <em>understand language</em>, could not generate multiple
pages of coherent text, that they are merely <a
href="https://dl.acm.org/doi/10.1145/3442188.3445922">stochastic
parrots</a> and cannot do useful knowledge work.</p>
<p>Well, now we all know how that turned out. Today, language models are
transforming the nature of software engineering, and approaching the
creation of serious economic value.</p>
<p>I want to hear less ‚Äúlet‚Äôs compromise on a more practical [totally
unimpressive] approach‚Äù and more ‚Äúlet‚Äôs build Dyson spheres.‚Äù To build
today‚Äôs language models, we had to take a leap of faith. Let us take a
leap of faith again and solve the hard problem of world modeling
head-on. Let‚Äôs solve the engineering and energy problems along the way.
If it requires fundamental advances in hardware, so be it. The energy is
out there ‚Äì the Sun gives it to us for free. Let‚Äôs cover Nevada in solar
panels, or send them into space, or build a literal Dyson sphere. We can
do this. As a civilization, we must continue to consume more energy in
order to advance.</p>
<p>Feel it in the air. We are on the cusp of an incredible technology.
It is waiting for you to invent it.</p>
<p><span style="color: gray;"><em>If this resonates, feel free to
<a href="/contact">drop me a line</a>.</em></span></p>

        </div>
        <div class="menu_bar" id="rightspace"></div>
    </div>
</body>
</html>