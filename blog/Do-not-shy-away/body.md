<p style="margin-top: 0">Do not shy away from the hard problems in deep learning.<p>

We have made tremendous progress as a society on modeling the sequence of language. By scaling next-token prediction, we have achieved highly useful language-based assistants. Now, there lies a new, even more exciting frontier in AI: modeling the sequence of the world. At each moment in time, humans move through a rich, multimodal sequence of sensory stimuli and motor control. Taking inspiration from the success of next-token prediction, what if we could train next-_moment_ prediction models which learn to generate streams of continuous visuomotor data?

A highly realistic, realtime world simulator. One day, at hundreds of frames per second, predicting photorealistic egocentric images and raw audio, conditioned on a long, dense history of frames spanning hours – maybe years – into the past. Importantly, predicting not just observations, but also dextrous human motor actions and _speech_ in one, unified model. I take the extreme position here and advocate for the ultimate instantiation of sequence modeling: predict the data. All of it.

What lies before us is the hardest problem in deep learning. Importantly, this model subsumes language models. This is not an argument _against_ today's language models, however – it is instead a full embrace of their success. Let us take loving inspiration from language modeling and apply its best ideas to the most general case: end-to-end autoregressive modeling of human experience, behavior, and language – in the raw, continuous data space. And, yes, supervised pretraining followed by reinforcement learning, except now the environment is planet Earth. If you like working on language models, you'll _love_ working on this.

Yes, if successful, the training and inference of such a model would require more compute than today's frontier language models. _Orders of magnitude more._

"Um, is that even possible?", I hear you say. "We will not be able to fabricate enough chips to train such a model, let alone generate enough power across the whole planet to keep them running. Why not opt for a more efficient approach? We don't _have_ to do realtime inference, we don't _have_ to store every frame in the context, we can just use the diffusion model as a 'rendering layer', we can just use a language model for high level planning, we could take a hierarchical System 1 System 2 approach, you shouldn't take the bitter lesson so literally, besides, video prediction will never achieve realistic physics, oh and _world models_ are just supposed to predict in latent sp-"

I say we do it anyway. Believe it or not, people said similar things about language models when GPT-3 came out. That large language models would be far too computationally expensive and cost far too much energy to be practical. That GPT models would never power NPCs in video games because they could never be run locally or be served fast enough. That they don't _understand language_, could not generate multiple pages of coherent text, that they are merely stochastic parrots and cannot do useful knowledge work.

Well, now we all know how that turned out. Today, language models are transforming the nature of software engineering, literally churning out entire C compilers, and approaching the creation of serious economic value.

I want to hear less "let's compromise on a more practical [totally unimpressive] approach" and more "let's build Dyson spheres." To build today's language models, we had to take a leap of faith. Let us take a leap of faith again and solve the hard problem of world modeling head-on. Let's solve the engineering and energy problems along the way. If it requires fundamental advances in hardware, so be it. The energy is out there – the Sun gives it to us for free. Let's cover Nevada in solar panels, or send them into space, or build a literal Dyson sphere. We can do this. As a civilization, we must continue to consume more energy in order to advance.

Feel it in the air. We are on the cusp of an incredible technology. It is waiting for you to invent it.


<span style="color: gray;">_If this resonates, feel free to <a href="/contact">drop me a line</a>._</span>
