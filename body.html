<div id="p5" class="fullwidthbox"></div>

<p>Hello, I'm Julian!</p>
<p>I'm an undergrad at Stanford interested in large scale models of the human experience.</p>

<p>Most recently, I was a researcher at <a href="https://www.worldlabs.ai/">World Labs</a>. Before that, I co-created <a href="https://oasis-model.github.io/">Oasis</a>, the first realtime world model with a playable demo. And prior to that, I optimized inference for the first era of large language models at <a href="https://cohere.com/">Cohere</a> and <a href="https://www.databricks.com/research/mosaic">MosaicML</a>.</p>


<h3>Highlighted Work</h3>
<table>
	<tr>
		<td><img src="/img/world_model_eval.gif" class="thumbnail"></td>
		<td>
			<b>WorldGym: World Model as An Environment for Policy Evaluation</b><br>
			<b>Julian Quevedo</b>, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, <a href="https://sherryy.github.io/">Sherry Yang</a>
			<br>
			[<a href="https://arxiv.org/abs/2506.00613">arxiv</a>] [<a href="https://world-model-eval.github.io/abstract">website</a>] [<a href="https://github.com/world-model-eval/world-model-eval">code</a>]
			<!-- <br><i>World Modeling Workshop 2026</i> -->
		</td>
	</tr>
	<tr>
		<td><img src="/img/rtfm.gif" class="thumbnail"></td>
		<td>Real-Time Frame Model<br>
		<a href="https://www.worldlabs.ai/">World Labs</a><br>
		[<a href="https://www.worldlabs.ai/blog/rtfm">blog</a>]
		</td>
	</tr>
	<tr>
		<td><img src="/img/oasis.gif" class="thumbnail"></td>

		<td>
			<b>Oasis: A Universe in a Transformer</b><br>
			<a href="https://www.decart.ai/">Decart</a> & Quevedo, et al.<br>
			[<a href="https://oasis-model.github.io/">blog</a>] [<a href="https://oasis.decart.ai/welcome">demo</a>] [<a href="https://github.com/etched-ai/open-oasis">code</a>]
			<br>Press: <a href="https://techcrunch.com/2024/10/31/decarts-ai-simulates-a-real-time-playable-version-of-minecraft/">TechCrunch</a>
		</td>
	</tr>
	<tr>
		<td><img src="/img/quantization.jpg" class="thumbnail"></td>

		<td>
			<b>Serving Quantized LLMs on NVIDIA H100 Tensor Core GPUs</b><br>
			Nikhil Sardana, <b>Julian Quevedo</b>, Daya Khudia<br>
			[<a href="https://www.databricks.com/blog/serving-quantized-llms-nvidia-h100-tensor-core-gpus">blog</a>]
		</td>
		
	</tr>

	<tr>
		<td><img src="/img/best_practices.png" class="thumbnail"></td>
		<td>
			<b>LLM inference performance engineering: Best practices</b><br>
			Megha Agarwal, Asfandyar Qureshi, Nikhil Sardana, Linden Li, <b>Julian Quevedo</b>, Daya Khudia<br>
			[<a href="https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices">blog</a>]
		</td>
	</tr>
</table>

<p><a href="https://github.com/julian-q">github</a> | <a href="https://x.com/julianhquevedo">twitter</a> | <a href="https://scholar.google.com/citations?user=VktYHN8AAAAJ&hl=en&scioq=Serving+Quantized+LLMs+on+NVIDIA+H100+Tensor+Core+GPUs#">google scholar</a> | <a href="https://www.linkedin.com/in/julian-q/">linkedin</a>
<p>I would love to hear from you: julianq@stanford.edu</p>